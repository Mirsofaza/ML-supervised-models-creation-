{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook demonstrates\n",
        "1. Manual implementation of Adaboost.M1 algorithm for classification\n",
        "2. Use library to implement Adaboost.M1 algorithm for classification\n",
        "3. Manual implementation of general boosting (Gradient boosting) algorithm for regression.\n",
        "\n",
        "Understanding if we can ensemble any models.\n",
        "\n",
        "4. Use library to implement Gradient boosting algorithm for regression"
      ],
      "metadata": {
        "id": "01ALo-lYTKjP"
      },
      "id": "01ALo-lYTKjP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part I: Manual implementation of Adaboost.M1 algorithm for classification\n",
        "\n",
        "1.1 A simple example for synthetic data for classification"
      ],
      "metadata": {
        "id": "cafP6ZR2TheK"
      },
      "id": "cafP6ZR2TheK"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Create a synthetic, more complex dataset by using make_classificaiton\n",
        "# 20 input features, only 15 are useful. flip_y 3% of the samples will have their class label randomly changed\n",
        "# A class_sep of 0.5 suggests moderate overlap, making the classification problem neither too easy nor too hard\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5,\n",
        "                           n_classes=3, flip_y=0.03, class_sep=0.5)\n",
        "\n",
        "# Convert to binary classification (Class 0 vs. Class 1 and 2)\n",
        "y_binary = (y > 0).astype(int)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_(\n",
        "\n",
        "# AdaBoost parameters\n",
        "n_estimators = 5\n",
        "learning_rate = 0.1 # nu in slide 41\n",
        "\n",
        "# Initialize weights, np.full create an array where each element has the same value, specifically (1 / len(y_train))\n",
        "weights = np.full(\n",
        "# Store estimators and their weights\n",
        "estimators = []\n",
        "alpha_m = []\n",
        "\n",
        "for m in range(n_estimators):\n",
        "    # Train a weak learner\n",
        "    weak_learner =  # Stump\n",
        "    weak_learner. # Need to specify weights for each sample\n",
        "    estimators.append()\n",
        "\n",
        "    # Predict and calculate error for traing data\n",
        "    predictions = weak_learner.\n",
        "    miss = [ for x in (]  # Indicator of incorrect predictions\n",
        "    error_m =\n",
        "\n",
        "    # Calculate alpha (estimator weight)\n",
        "    alpha = learning_rate *   # need to add a small value in the denominator\n",
        "    alpha_m.append()\n",
        "\n",
        "    # Update weights\n",
        "    weights *=\n",
        "    weights /= sum(weights) # normalization is optional\n",
        "\n",
        "# Make predictions\n",
        "final_predictions = sum( for alpha, estimator in zip(alpha_m, estimators))\n",
        "final_predictions = np.sign(final_predictions)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy = accuracy_score( )\n",
        "print(f\"Accuracy: { }\")\n"
      ],
      "metadata": {
        "id": "Farw2jCtJo-v"
      },
      "id": "Farw2jCtJo-v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for alpha, estimator in zip(alpha_m, estimators):\n",
        "  print(alpha, estimator.predict(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_K0axyiT20j",
        "outputId": "6bb771c9-acb8-4329-a47c-56fd1cde989f"
      },
      "id": "H_K0axyiT20j",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.07138440620268942 [1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 0 0 0 0\n",
            " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1\n",
            " 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1\n",
            " 1 1 0 0 1 0 0 1 0 1 0 1 1 1 1]\n",
            "0.06973504298147712 [1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 0 1 1 0\n",
            " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1\n",
            " 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1]\n",
            "0.061523916753945675 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "0.05537152507965455 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "0.04983437257262665 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2 The example for Slide 31\n",
        "\n",
        "First we generate the dataset according to the slide"
      ],
      "metadata": {
        "id": "ciYCDavdTyvT"
      },
      "id": "ciYCDavdTyvT"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import chi2\n",
        "\n",
        "# Parameters\n",
        "n_train = 2000\n",
        "n_test = 10000\n",
        "n_features = 10\n",
        "chi2_val = chi2.ppf(0.5, df = n_features) #the value of the quantile function ppf-percent point function\n",
        "\n",
        "# Generate random standard normal X\n",
        "np.random.seed(42)\n",
        "X_train = np.random.normal(size=(n_train, n_features)) # (2000, 10)\n",
        "X_test = np.random.normal(size=(n_test, n_features)) # (10000, 10)\n",
        "\n",
        "# Generate Y according to the formula\n",
        "y_train = np.where(np.sum(X_train**2, axis=1) > chi2_val, 1, -1)\n",
        "y_test = np.where(np.sum(X_test**2, axis=1) > chi2_val, 1, -1)\n"
      ],
      "metadata": {
        "id": "0BK8cCWXM7Jh"
      },
      "id": "0BK8cCWXM7Jh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second, please answer the following questoin."
      ],
      "metadata": {
        "id": "RikT70e9UBD9"
      },
      "id": "RikT70e9UBD9"
    },
    {
      "cell_type": "code",
      "source": [
        "# Question: Based on the Adaboost.M1 example for Iris data, please provide a manual\n",
        "# implementation of AdaBoost.M1 for the data generated above\n",
        "# Please print\n",
        "# AdaBoost Accuracy:\n",
        "# Single Stump Accuracy:\n",
        "# Large CART Accuracy:\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# AdaBoost parameters\n",
        "n_estimators = 50\n",
        "learning_rate = 1.5\n",
        "\n",
        "# Initialize weights\n",
        "weights =\n",
        "\n",
        "# Store estimators and their weights\n",
        "estimators = []\n",
        "alpha_m = []\n",
        "\n",
        "for m in range(n_estimators):\n",
        "    # Train a weak learner (two-node CART)\n",
        "    stump =\n",
        "    stump.\n",
        "    ...\n",
        "\n",
        "    # Predict and calculate error\n",
        "    stump_pred =\n",
        "    miss =\n",
        "    ...\n",
        "\n",
        "    # Update weights\n",
        "    weights\n",
        "\n",
        "# AdaBoost prediction\n",
        "adaboost_pred =\n",
        "adaboost_pred =\n",
        "\n",
        "# Evaluate performance\n",
        "ada_accuracy =\n",
        "print(f\"AdaBoost Accuracy: {}\")\n",
        "\n",
        "# Single stump\n",
        "single_stump =\n",
        "single_stump.\n",
        "stump_accuracy =\n",
        "\n",
        "# Larger CART tree\n",
        "large_cart =   # Full depth by specifying max_depth=None\n",
        "large_cart.\n",
        "cart_accuracy =\n",
        "\n",
        "print(f\"Single Stump Accuracy: {}\")\n",
        "print(f\"Large CART Accuracy: {}\")"
      ],
      "metadata": {
        "id": "yLoROT_4SvDK"
      },
      "id": "yLoROT_4SvDK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Third, let's use AdaBoostClassifier to implement it."
      ],
      "metadata": {
        "id": "X99y8muGU1zf"
      },
      "id": "X99y8muGU1zf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2db2fd9",
      "metadata": {
        "id": "a2db2fd9"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from scipy.stats import chi2\n",
        "\n",
        "\n",
        "# Single Stump\n",
        "stump =\n",
        "stump.\n",
        "y_pred_stump =\n",
        "accuracy_stump =\n",
        "print('Accuracy using Single Stump:', )\n",
        "\n",
        "# 244-node CART\n",
        "# he 244-node CART can be further customized\n",
        "# by adjusting hyperparameters like the max_depth, min_samples_split, or min_samples_leaf\n",
        "tree = # 244-node tree\n",
        "tree.\n",
        "y_pred_tree =\n",
        "# Estimate the number of correct predictions to the total number of predictions\n",
        "accuracy_tree =\n",
        "print('Accuracy using 244-node CART:', )\n",
        "\n",
        "# AdaBoost with a Single Stump\n",
        "# The code is only good for binary classification problem.\n",
        "# base_estimator is chosen to be Single Stump. But with boosting, its performance excels\n",
        "# the number of weak learners (or estimators) to train iteratively is 50\n",
        "ada_stump = .(estimator=, n_estimators=)\n",
        "ada_stump.\n",
        "y_pred_ada_stump =\n",
        "accuracy_ada_stump =\n",
        "print('Accuracy using AdaBoost with Single Stump:', )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fourth, we visualize the results."
      ],
      "metadata": {
        "id": "mL0GFbq9U9cV"
      },
      "id": "mL0GFbq9U9cV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7653f4ae",
      "metadata": {
        "id": "7653f4ae"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Plotting Test Error vs. Iterations for Single Stump\n",
        "plt.axhline(y=, color='b', linestyle='-', label='Single Stump')\n",
        "\n",
        "# Plotting Test Error vs. Iterations for 244-node CART\n",
        "plt.axhline(y=, color='g', linestyle='-', label='244-node CART')\n",
        "\n",
        "# Plotting Test Error vs. Iterations for AdaBoost\n",
        "# obtain predictions from the model at each stage of the training process by .staged_predict\n",
        "staged_accuracy_ada_stump = [ for y_pred in ada_stump.]\n",
        "plt.plot([ for acc in ], color='r', label='AdaBoost with Single Stump')\n",
        "\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Test Error')\n",
        "plt.title('Test Error vs. Iterations')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part II: General Boosting algorithm (Gradient boosting) for regression\n",
        "\n",
        "First, we manually implement gradient boosting"
      ],
      "metadata": {
        "id": "Lm2k0O3JUmV8"
      },
      "id": "Lm2k0O3JUmV8"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target #X-(20640, 8) y-(20640,)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Parameters\n",
        "n_estimators = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Initialize model\n",
        "f_m_x_train = np.full() # Initial prediction for train set, len(y_train)\n",
        "f_m_x_test = np.full(  # Initial prediction for test set\n",
        "\n",
        "trees = []\n",
        "# Gradient Boosting - Slide 38 Chapter 14\n",
        "for m in range(n_estimators):\n",
        "    # Compute residuals\n",
        "    residuals =\n",
        "\n",
        "    # Fit a tree to the residuals\n",
        "    # Each time, the tree is trained on the residual estimated from the previous iteration\n",
        "    tree =\n",
        "    tree.\n",
        "    trees.\n",
        "\n",
        "    # Update model for both train and test set\n",
        "    f_m_x_train +=\n",
        "\n",
        "# Update model for test set using all trees\n",
        "for tree in trees:\n",
        "  f_m_x_test +=\n",
        "\n",
        "# Evaluate performance on test data\n",
        "mse = mean_squared_error(\n",
        "print(f\"Mean Squared Error: { }\")\n"
      ],
      "metadata": {
        "id": "4drJQTI8EAFK"
      },
      "id": "4drJQTI8EAFK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second, please answer the question below. From this example, you will experience if ensemble any models would always be beneficial or not.\n"
      ],
      "metadata": {
        "id": "9rRyqKVwVEd0"
      },
      "id": "9rRyqKVwVEd0"
    },
    {
      "cell_type": "code",
      "source": [
        "# Question: Based on the way of implementing gradient boosting above,\n",
        "# Please manually implement the gradient boosting of linear models.\n",
        "# Here, the weak learner is the linear regression model\n",
        "\n",
        "# Compare the results with just one linear regression and discuss if there are any advantages doing ensemble\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Parameters\n",
        "n_estimators = 50\n",
        "learning_rate = 1.5\n",
        "\n",
        "# Initialize model\n",
        "f_m_x_train = np.zeros(  # Initial prediction for train set\n",
        "f_m_x_test = np.zeros(    # Initial prediction for test set\n",
        "\n",
        "models = []\n",
        "# Ensemble of Linear Regression Models\n",
        "for m in range(n_estimators):\n",
        "    # Compute residuals\n",
        "    residuals =\n",
        "\n",
        "    # Fit a linear regression model to the residuals\n",
        "    model =\n",
        "    model.\n",
        "    models.append(\n",
        "    # Update model\n",
        "    f_m_x_train\n",
        "\n",
        "# Update model for test set using all trees\n",
        "for m in models:\n",
        "    f_m_x_test +=\n",
        "\n",
        "# Evaluate performance on test data\n",
        "mse = mean_squared_error(\n",
        "print(f\"Mean Squared Error: {}\")\n"
      ],
      "metadata": {
        "id": "hFD7K1zfFg7G"
      },
      "id": "hFD7K1zfFg7G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Third, let's implement gradient boosting by using GradientBoostingRegressor"
      ],
      "metadata": {
        "id": "oUSAuZNXVjoZ"
      },
      "id": "oUSAuZNXVjoZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0924a443",
      "metadata": {
        "id": "0924a443"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Load the California housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Gradient Boosting model\n",
        "# A GradientBoostingRegressor is created and fitted to the training data.\n",
        "# This model uses 100 trees (n_estimators=100) and a learning rate of 0.1. max depth 3\n",
        "regressor =\n",
        "regressor.\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "predictions = regressor.\n",
        "mse = mean_squared_error(\n",
        "print(f\"Mean Squared Error: {}\")\n",
        "\n",
        "# Create and train a Linear Regression model\n",
        "linear_regressor =\n",
        "linear_regressor.\n",
        "\n",
        "# Make predictions with the Linear Regression model\n",
        "linear_predictions =\n",
        "\n",
        "# Evaluate the Linear Regression model\n",
        "linear_mse = mean_squared_error(\n",
        "print(f\"Linear Regression Mean Squared Error: {}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}