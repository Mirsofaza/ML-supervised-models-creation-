{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **This example demonstrates how to perform Bayesian logistic regression on Iris flower dataset.**\n",
        "1. Perform one round of regular logistic regression to generate prior\n",
        "2. Perform Bayesian logistic regression"
      ],
      "metadata": {
        "id": "4LHVvF4QvmbI"
      },
      "id": "4LHVvF4QvmbI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load data and prepare training dataset X and y**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EgkOBSzFmRPI"
      },
      "id": "EgkOBSzFmRPI"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "853cdf50",
      "metadata": {
        "id": "853cdf50"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, :2]  # we only take the first two features.\n",
        "y = iris.target\n",
        "\n",
        "# use the first two inputs with class labels 0 or 1\n",
        "X = X[y<2, :2] # X should be 100X2\n",
        "y = y[y<2]\n",
        "\n",
        "# Prepare the input for testing or applicaiton data\n",
        "h = 0.02  # step size in the mesh\n",
        "x1_min, x1_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
        "x2_min, x2_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
        "xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, h), np.arange(x2_min, x2_max, h))\n",
        "\n",
        "# Prepare the design matrix for input of testing data. 31635X3, each row in Phi_test is phi.T in the slide\n",
        "Phi_test = np."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Perform logistic regression with L2 regularization to obtain the MAP of mN (Slide 63).**"
      ],
      "metadata": {
        "id": "7RDBQ0DBkFt3"
      },
      "id": "7RDBQ0DBkFt3"
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression with L2 penalty is equivalent to MAP on omega with m0 = 0, S0 = 1/alpha * I. See why in Slide 63\n",
        "# Estimate the prior for w coefficients w~N(m0,N0)\n",
        "# S0 can be an identity matrix scaled by alpha\n",
        "alpha = 2\n",
        "S0 = # an identity matrix scaled by alpha. X size 100X1 We have w0, w1, w2 three parameters.\n",
        "# m0 is usually zero mean. But we do not need it in the code!\n",
        "# We just need to perform a logistic regression with L2 regularization to obtain prior for the Bayesian estimation\n",
        "classifier =  #if alpha=1, it is default logistic regression\n",
        "\n",
        "# get the MAP of omega\n",
        "w0 =  # intercept from the previous results fitted\n",
        "w1 =\n",
        "\n",
        "\n",
        "m_N = #shape (3,)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iKygMJP-i50F"
      },
      "id": "iKygMJP-i50F",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Perform Bayesian Logistic Regression based on Slides 66-67, 70**\n"
      ],
      "metadata": {
        "id": "sPp_FuM4lWN4"
      },
      "id": "sPp_FuM4lWN4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we need to run Laplace approximation to estimate the posterior of w ~ N(m_N, A^-1)"
      ],
      "metadata": {
        "id": "G7o4mJuA1VFd"
      },
      "id": "G7o4mJuA1VFd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "274aa8c0",
      "metadata": {
        "id": "274aa8c0"
      },
      "outputs": [],
      "source": [
        "def logistic_function(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "# Design matrix Phi- based on training data. The matrix of input features (with an extra column of ones to account\n",
        "# for the intercept term) for the samples where y < 1\n",
        "Phi = # 50X3 matrix. Essentially, each row in Phi is phi.T in the slide\n",
        "Phi =  # extract label 0 class along the rows\n",
        "\n",
        "# Compute the scalar value for each row in Phi\n",
        "R_nn =  #shape (50,)\n",
        "\n",
        "# Since R is a 1D array, we reshape it to a 2D column vector for the following operations\n",
        "R_nn =  #50X1 shape\n",
        "\n",
        "# Compute the Hessian of the negative log posterior\n",
        "# Scale each row in Phi by the corresponding value in R\n",
        "R = # Use R_nn to create a diagonal matrix\n",
        "A =\n",
        "\n",
        "# Invert the Hessian to get the covariance matrix of the Laplace approximation\n",
        "S_N =\n",
        "\n",
        "print(\"m_N (MAP estimate):\", )\n",
        "print(\"S_N (Covariance matrix of the Laplace approximation):\", )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we use the posterior of w estimated by Laplace approximation to estimate the preditive distribution of class for the inputs of testing/application data xx1, xx2"
      ],
      "metadata": {
        "id": "n2j-sDSS1peC"
      },
      "id": "n2j-sDSS1peC"
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import norm\n",
        "\n",
        "# Compute the predictive probabilities for the test set using slide 71\n",
        "a_values = # should include 31635 elements\n",
        "# Compute b. Note phi is one row in Phi_test, which is the transpose version of the phi in the slide\n",
        "b_values = # should includ 31635 elements in an array\n",
        "\n",
        "Z_values =\n",
        "\n",
        "# Reshape Z to the shape of the meshgrid for contour plotting\n",
        "Z = Z_values.reshape(xx1.shape)\n",
        "\n",
        "# Plot\n",
        "cm = plt.cm.RdBu\n",
        "ax = plt.subplot(1,1,1)\n",
        "ax.set_xlim(xx1.min(), xx1.max())\n",
        "ax.set_ylim(xx2.min(), xx2.max())\n",
        "ax.set_xticks(())\n",
        "ax.set_yticks(())\n",
        "ax.contourf(, cmap=cm, alpha=0.8)\n",
        "\n",
        "ax.scatter(X[y==0, 0], X[y==0, 1], c='tab:red', edgecolor='k', label=iris.target_names[0])\n",
        "ax.scatter(X[y==1, 0], X[y==1, 1], c='tab:blue', edgecolor='k', label=iris.target_names[1])\n",
        "\n",
        "plt.xlabel(\"Sepal length\")\n",
        "plt.ylabel(\"Sepal width\")\n",
        "plt.title('Bayesian Logistic Regression')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iIdnPkoF1nyJ"
      },
      "id": "iIdnPkoF1nyJ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}