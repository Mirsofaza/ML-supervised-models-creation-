{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d4bd195",
      "metadata": {
        "id": "4d4bd195"
      },
      "outputs": [],
      "source": [
        "from scipy import linalg\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from matplotlib import colors\n",
        "\n",
        "#importing the Linear Discriminant Analysis function from the Scikit-learn library\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "# importing the Quadratic Discriminant Analysis function from the Scikit-learn library.\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "\n",
        "# #############################################################################\n",
        "# Colormap- essentially a mapping from numbers to colors\n",
        "#  the red component starts at full intensity and gradually decreases to 70% intensity\n",
        "# as it moves from the lower to the upper end of the scale. Note: this is not about location.\n",
        "# The same pattern is followed for the green and blue components.\n",
        "cmap = colors.LinearSegmentedColormap(\n",
        "    \"red_blue_classes\",\n",
        "    {\n",
        "        \"red\": [(0, 1, 1), (1, 0.7, 0.7)],\n",
        "        \"green\": [(0, 0.7, 0.7), (1, 0.7, 0.7)],\n",
        "        \"blue\": [(0, 0.7, 0.7), (1, 1, 1)],\n",
        "    },\n",
        ")\n",
        "# Registering the new colormap with Matplotlib (which will be imported as plt),\n",
        "# so that it can be used later on in the script for creating plots\n",
        "#plt.cm.register_cmap(cmap=cmap) # You should disable this line if you want to run this code again\n",
        "\n",
        "\n",
        "# #############################################################################\n",
        "# Dataset 1: Generate datasets with 600 samples (300 for each class) and 2 features.\n",
        "# The samples are drawn from a 2-dimensional Gaussian distribution.\n",
        "# The labels for the first 300 samples are all zeros, and\n",
        "# the labels for the second 300 samples are all ones.\n",
        "# Note that the we have the same C (scale of the randomness) for class 1 and class 0\n",
        "def dataset_fixed_cov():\n",
        "    \"\"\"Generate 2 Gaussians samples with the same covariance matrix\"\"\"\n",
        "    n, dim = 300, 2\n",
        "    np.random.seed(0)\n",
        "    C = np.array([[0.0, -0.23], [0.83, 0.23]])\n",
        "    X = np.r_[ # stacking rows by rows\n",
        "        np.dot(np.random.randn(n, dim), C), # input data X for class 1\n",
        "        np.dot(np.random.randn(n, dim), C) + np.array([1, 1]),\n",
        "    ]\n",
        "    y = np.hstack((np.zeros(n), np.ones(n))) # labels 0 for one class and 1 for the other class\n",
        "    return X, y\n",
        "\n",
        "# Dataset 2: Generate another 600 samples. The key difference is that C is not the same for class 0 and class 1\n",
        "def dataset_cov():\n",
        "    \"\"\"Generate 2 Gaussians samples with different covariance matrices\"\"\"\n",
        "    n, dim = 300, 2\n",
        "    np.random.seed(0)\n",
        "    C = np.array([[0.0, -1.0], [2.5, 0.7]]) * 2.0 # Create a 2X2 matrix scaled by 2\n",
        "    X = np.r_[\n",
        "        np.dot(np.random.randn(n, dim), C),\n",
        "        np.dot(np.random.randn(n, dim), C.T) + np.array([1, 4]),\n",
        "    ]\n",
        "    y = np.hstack((np.zeros(n), np.ones(n))) # 0, 0, 0, .... 1, 1, 1 vector\n",
        "    return X, y\n",
        "\n",
        "\n",
        "# #############################################################################\n",
        "# Plot functions\n",
        "def plot_data(classifier, X, y, y_pred, fig_index):\n",
        "    splot = plt.subplot(2, 2, fig_index)\n",
        "    if fig_index == 1:\n",
        "        plt.title(\"Linear Discriminant Analysis\")\n",
        "        plt.ylabel(\"Data with\\n fixed covariance\")\n",
        "    elif fig_index == 2:\n",
        "        plt.title(\"Quadratic Discriminant Analysis\")\n",
        "    elif fig_index == 3:\n",
        "        plt.ylabel(\"Data with\\n varying covariances\")\n",
        "\n",
        "    tp = y == y_pred  # True Positive\n",
        "    tp0, tp1 = tp[y == 0], tp[y == 1]\n",
        "    X0, X1 = X[y == 0], X[y == 1]\n",
        "    X0_tp, X0_fp = X0[tp0], X0[~tp0]\n",
        "    X1_tp, X1_fp = X1[tp1], X1[~tp1]\n",
        "\n",
        "    # class 0: dots\n",
        "    plt.scatter(X0_tp[:, 0], X0_tp[:, 1], marker=\".\", color=\"red\")\n",
        "    plt.scatter(X0_fp[:, 0], X0_fp[:, 1], marker=\"x\", s=20, color=\"#990000\")  # dark red\n",
        "\n",
        "    # class 1: dots\n",
        "    plt.scatter(X1_tp[:, 0], X1_tp[:, 1], marker=\".\", color=\"blue\")\n",
        "    plt.scatter(\n",
        "        X1_fp[:, 0], X1_fp[:, 1], marker=\"x\", s=20, color=\"#000099\"\n",
        "    )  # dark blue\n",
        "\n",
        "    # class 0 and 1 : areas\n",
        "    nx, ny = 200, 100\n",
        "    x_min, x_max = plt.xlim()\n",
        "    y_min, y_max = plt.ylim()\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, nx), np.linspace(y_min, y_max, ny))\n",
        "    Z = classifier.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z[:, 1].reshape(xx.shape)\n",
        "    plt.pcolormesh(\n",
        "        xx, yy, Z, cmap=\"red_blue_classes\", norm=colors.Normalize(0.0, 1.0), zorder=0\n",
        "    )\n",
        "    plt.contour(xx, yy, Z, [0.5], linewidths=2.0, colors=\"white\")\n",
        "\n",
        "    # means\n",
        "    plt.plot(\n",
        "        classifier.means_[0][0],\n",
        "        classifier.means_[0][1],\n",
        "        \"*\",\n",
        "        color=\"yellow\",\n",
        "        markersize=15,\n",
        "        markeredgecolor=\"grey\",\n",
        "    )\n",
        "    plt.plot(\n",
        "        classifier.means_[1][0],\n",
        "        classifier.means_[1][1],\n",
        "        \"*\",\n",
        "        color=\"yellow\",\n",
        "        markersize=15,\n",
        "        markeredgecolor=\"grey\",\n",
        "    )\n",
        "\n",
        "    return splot\n",
        "\n",
        "\n",
        "def plot_ellipse(splot, mean, cov, color):\n",
        "    v, w = linalg.eigh(cov)\n",
        "    u = w[0] / linalg.norm(w[0])\n",
        "    angle = np.arctan(u[1] / u[0])\n",
        "    angle = 180 * angle / np.pi  # convert to degrees\n",
        "    # filled Gaussian at 2 standard deviation\n",
        "    ell = mpl.patches.Ellipse(\n",
        "        mean,\n",
        "        2 * v[0] ** 0.5,\n",
        "        2 * v[1] ** 0.5,\n",
        "        180 + angle,\n",
        "        facecolor=color,\n",
        "        edgecolor=\"black\",\n",
        "        linewidth=2,\n",
        "    )\n",
        "    ell.set_clip_box(splot.bbox)\n",
        "    ell.set_alpha(0.2)\n",
        "    splot.add_artist(ell)\n",
        "    splot.set_xticks(())\n",
        "    splot.set_yticks(())\n",
        "\n",
        "\n",
        "def plot_lda_cov(classifier, splot): #plotting the covariance ellipses\n",
        "    plot_ellipse(splot, classifier.means_[0], classifier.covariance_, \"red\")\n",
        "    plot_ellipse(splot, classifier.means_[1], classifier.covariance_, \"blue\")\n",
        "\n",
        "\n",
        "def plot_qda_cov(classifier, splot):\n",
        "    plot_ellipse(splot, classifier.means_[0], classifier.covariance_[0], \"red\")\n",
        "    plot_ellipse(splot, classifier.means_[1], classifier.covariance_[1], \"blue\")\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 8), facecolor=\"white\")\n",
        "plt.suptitle(\n",
        "    \"Linear Discriminant Analysis vs Quadratic Discriminant Analysis\",\n",
        "    y=0.98,\n",
        "    fontsize=15,\n",
        ")\n",
        "# i - index starting from 0\n",
        "# enumerate (X,y) from all data generated. When i = 0, (X,y) come from dataset_fixed_cv. when i=1, from dataset_cov()\n",
        "for i, (X, y) in enumerate([dataset_fixed_cov(), dataset_cov()]):\n",
        "    print('i=',i, '  (X,y): ', (X.shape, y.shape))\n",
        "    # Linear Discriminant Analysis\n",
        "    # first estimating the mean vectors and the covariance matrices for each class from the training dataset. Saved in lda\n",
        "    # Perform Fisher's Discriminant analysis considering maximizing between-class difference and minimizing within class variance\n",
        "    classifier = # You should let classifier store covariance by setting store_covariance=True. Otherwise, you will have trouble with plot_lda_cov()\n",
        "    y_pred =\n",
        "    splot = plot_data(, fig_index=2 * i + 1) # Use the plot function defined above\n",
        "    plot_lda_cov(classifier, splot) #plotting the covariance ellipses, function defined above\n",
        "    plt.axis(\"tight\") #djusts the axes of the plot such that all the data is visible and the plot area is maximized\n",
        "\n",
        "    # Quadratic Discriminant Analysis\n",
        "    classifier =  # You should let classifier store covariance by setting store_covariance=True. Otherwise, you will have trouble with plot_qda_cov()\n",
        "    y_pred =\n",
        "    splot = plot_data(, fig_index=2 * i + 2)\n",
        "    plot_qda_cov(classifier, splot)\n",
        "    plt.axis(\"tight\")\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(top=0.92)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}