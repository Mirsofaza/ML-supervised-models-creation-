{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **This example demonstrates how to use AIC or BIC to make selection of a model**\n",
        "We first do it manually. Then check out a quick approach to estimate them."
      ],
      "metadata": {
        "id": "Y4WRznyuqXI0"
      },
      "id": "Y4WRznyuqXI0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Google Drive to the colab."
      ],
      "metadata": {
        "id": "BwyZla6pqgLo"
      },
      "id": "BwyZla6pqgLo"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1i2zBREE2LI",
        "outputId": "6b085818-876a-407b-de0f-5543288a8fb1"
      },
      "id": "D1i2zBREE2LI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the data"
      ],
      "metadata": {
        "id": "vhNHk8J7qkME"
      },
      "id": "vhNHk8J7qkME"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import scipy\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score, log_loss\n",
        "import matplotlib.pyplot as plt\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# 1. Load dataset\n",
        "df = pd.read_csv(\"/content/gdrive/MyDrive/ESI5685/Phoneme Recognition.txt\")\n",
        "\n",
        "df = df[(df.g == 'aa') | (df.g == 'ao')] #filtering\n",
        "df.g = pd.get_dummies(df.g)['ao'] # convert aa or ao into dummies. 1- ao, 0-not ao but aa\n",
        "target = 'g' # which is equivalent to t\n",
        "features = [f'x.{i+1}' for i in range(256)]\n",
        "X, y = df[features].values, df[target].values # X size 1717X256, y is (1717,)"
      ],
      "metadata": {
        "id": "4xNe-r9qnhGN"
      },
      "id": "4xNe-r9qnhGN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head() # return the first 5 rows"
      ],
      "metadata": {
        "id": "VRLc0DacmYcv"
      },
      "id": "VRLc0DacmYcv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/gdrive/MyDrive/ESI5685/Phoneme Recognition.txt\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "cS4ZqX1gmZFm"
      },
      "id": "cS4ZqX1gmZFm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Reduce features in the model by selecting the first p features by multipling H with X.\n",
        "2. Manually estimate the corresponding AIC and BIC\n",
        "3. Determine which size of features leads to the lowest AIC and BIC, respectively."
      ],
      "metadata": {
        "id": "7C97mnfRw9R2"
      },
      "id": "7C97mnfRw9R2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9391250a",
      "metadata": {
        "id": "9391250a"
      },
      "outputs": [],
      "source": [
        "# 2. Implement dimensionality reduction\n",
        "def H_matrix(p, total_features):\n",
        "    \"\"\"Generate the projection matrix H that selects the first p features.\"\"\"\n",
        "    H = np.\n",
        "    for i in range(p):\n",
        "      H[i,i] =\n",
        "\n",
        "    return H\n",
        "\n",
        "total_features = 256 # all features in x1, x2... x256 in original dataset\n",
        "\n",
        "# 3. Train a logistic regression model and compute AIC\n",
        "aic_values = {}\n",
        "bic_values = {}\n",
        "possible_p_values = range(1, 256)\n",
        "\n",
        "for p in possible_p_values:\n",
        "    X_reduced =  #X: 1717X256, H: pX256\n",
        "    # This implementation will take quite a while. We can use solver='lbfgs' method to reduce computation\n",
        "    # resrouce for logistic regression. It is a\n",
        "    # quasi-Newton methods that approximates the Broyden-Fletcher-Goldfarb-Shanno (BFGS)\n",
        "    # algorithm using a limited amount of computer memory\n",
        "    # We need to set up a maximum iteration to avoid a warning of the solver convergence.\n",
        "    model =\n",
        "\n",
        "    # Predict probabilities\n",
        "    probas =\n",
        "\n",
        "    # Calculate log likelihood\n",
        "    # log_loss = y*log(probas)+(1-y)*log(1-probas)\n",
        "    loglikelihood =  # log_loss X n is equivalent to not to normalize it.\n",
        "\n",
        "    # Calculate AIC = 2k - 2ln(L), where k is number of parameters and L is likelihood of the model.\n",
        "    # The +1 accounts for the intercept term.\n",
        "    aic =\n",
        "    aic_values[p] =\n",
        "    # Calculate BIC = k * ln(n) - 2 * ln(L)\n",
        "    bic =\n",
        "    bic_values[p] =\n",
        "\n",
        "# 4. Select optimal p using AIC\n",
        "\n",
        "# Get the best key p (the one that gives the smallest AIC) from dictionary best_p_aic or best_p_bic\n",
        "# selects the key (best_p_aic/bic) for which aic_values.get returns the smallest value.\n",
        "# tell min to extract a comparison key from each input element\n",
        "best_p_aic = min( # aic_values and bic_values are dictionaries with p as keys.\n",
        "# Select optimal p using BIC\n",
        "best_p_bic = min(\n",
        "\n",
        "fig, ax = plt.subplots(2, 1, figsize=(10, 12))\n",
        "\n",
        "# AIC subplot\n",
        "ax[0].plot(, , marker='o', linestyle='-')\n",
        "ax[0].scatter(, , color='red', s=100, zorder=5)\n",
        "ax[0].set_title('AIC values vs. p')\n",
        "ax[0].set_xlabel('p')\n",
        "ax[0].set_ylabel('AIC')\n",
        "ax[0].grid(True)\n",
        "ax[0].set_xticks(list(aic_values.keys()))\n",
        "for i, v in aic_values.items():\n",
        "    ax[0].text(i, v + 0.5, \"%d\" %v, ha='center')\n",
        "\n",
        "# BIC subplot\n",
        "ax[1].plot(, , marker='o', linestyle='-')\n",
        "ax[1].scatter(, , color='red', s=100, zorder=5)\n",
        "ax[1].set_title('BIC values vs. p')\n",
        "ax[1].set_xlabel('p')\n",
        "ax[1].set_ylabel('BIC')\n",
        "ax[1].grid(True)\n",
        "ax[1].set_xticks(list(bic_values.keys()))\n",
        "for i, v in bic_values.items():\n",
        "    ax[1].text(i, v + 0.5, \"%d\" %v, ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        " # Inside the format string (f-string), expressions inside {}\n",
        "# are evaluated at runtime and then formatted using the format string syntax.\n",
        "print(f\"The best p value according to AIC is: { } with an AIC of { }\")\n",
        "print(f\"The best p value according to BIC is: { } with a BIC of { }\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Statsmodels library provides a quick way to estimate AIC and BIC**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FT2RVP7oxTFv"
      },
      "id": "FT2RVP7oxTFv"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Creating synthetic data\n",
        "np.random.seed(42)\n",
        "N = 100 # Number of samples\n",
        "X = np.random.randn(N, 5) # 5 features X0, X1, X2...X4\n",
        "# Ground truth: Only first 3 features X0, X1, X2 are related to y\n",
        "y = X[:, 0] + 2 * X[:, 1] - X[:, 2] + np.random.normal(0, 0.5, N)\n",
        "\n",
        "# Now we want to test if X0, X1... X4 should be included in the model\n",
        "# Defining function to fit linear model and return AIC and BIC\n",
        "\n",
        "# Comparing models with different subsets of features\n",
        "for i in range(1, 6):\n",
        "    # adds a column of ones to your input dataset X.\n",
        "    # This is done to include an intercept in the regression model. like np.column_stack((one, X))\n",
        "    X = sm.addConstant( # 100X6\n",
        "    # Fit a linear regression model using the Ordinary Least Squares (OLS)\n",
        "    # or Logistic Regression (Logit) method.\n",
        "\n",
        "    model = # fit model based on first i columns of X\n",
        "\n",
        "    print(f\"Model with first {i} features: AIC = { }, BIC = {}\")\n",
        "\n",
        "# Question: What conclusion can you draw based on the AIC and BIC results?"
      ],
      "metadata": {
        "id": "6QYYa4efKVrH"
      },
      "id": "6QYYa4efKVrH",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}